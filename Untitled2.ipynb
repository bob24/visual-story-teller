{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishanth\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import operator\n",
    "from unidecode import unidecode\n",
    "import h5py\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIS_DataReader:\n",
    "\n",
    "    def __init__(self, path_to_file=r'val.story-in-sequence.json'):\n",
    "        self.path_to_file = path_to_file\n",
    "\n",
    "    def create_word_frequency_document(self, path_to_json_file=r'word_frequencies.json'):\n",
    "\n",
    "        data = json.load(open(self.path_to_file))\n",
    "        annotations = data['annotations']\n",
    "\n",
    "        frequency = {}\n",
    "        for annotation in annotations:\n",
    "            sentence = annotation[0]['text'].split()\n",
    "            for word in sentence:\n",
    "                # proverka za brishenje na greski so zborovi vo unicode format(latinski zborovi)\n",
    "                if any(x.isupper() for x in unidecode(word)) == False:\n",
    "                    count = frequency.get(word, 0)\n",
    "                    frequency[word] = count + 1\n",
    "\n",
    "        sorted_frequency = sorted(frequency.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "        with open(path_to_json_file, 'w') as fp:\n",
    "            json.dump(sorted_frequency, fp)\n",
    "\n",
    "    def get_n_most_frequent_words(self, word_frequency_file=r'word_frequencies.json', vocabulary_size=10000):\n",
    "\n",
    "        data = json.load(open(word_frequency_file))\n",
    "        return data[0:vocabulary_size]\n",
    "\n",
    "    def generate_vocabulary(self, vocabulary_file=r'vist2017_vocabulary.json',\n",
    "                            word_frequency_file=r'word_frequencies.json', vocabulary_size=10000):\n",
    "\n",
    "        data = self.get_n_most_frequent_words(word_frequency_file, vocabulary_size)\n",
    "\n",
    "        idx_to_words = []\n",
    "        idx_to_words.append(\"<NULL>\")\n",
    "        idx_to_words.append(\"<START>\")\n",
    "        idx_to_words.append(\"<END>\")\n",
    "        idx_to_words.append(\"<UNK>\")\n",
    "\n",
    "        for element in data:\n",
    "            idx_to_words.append(element[0])\n",
    "\n",
    "        words_to_idx = {}\n",
    "        for i in range(len(idx_to_words)):\n",
    "            words_to_idx[idx_to_words[i]] = i\n",
    "\n",
    "        vocabulary = {}\n",
    "        vocabulary[\"idx_to_words\"] = idx_to_words\n",
    "        vocabulary[\"words_to_idx\"] = words_to_idx\n",
    "\n",
    "        with open(vocabulary_file, 'w') as fp:\n",
    "            json.dump(vocabulary, fp)\n",
    "\n",
    "    def get_max_sentence_length(self):\n",
    "\n",
    "        data = json.load(open(self.path_to_file))\n",
    "        annotations = data['annotations']\n",
    "        save_sent = []\n",
    "        max_sentence_length = 0\n",
    "\n",
    "        for annotation in annotations:\n",
    "            sentence = annotation[0]['text'].split()\n",
    "            length = len(sentence)\n",
    "            if (length > max_sentence_length):\n",
    "                max_sentence_length = length\n",
    "                save_sent = sentence\n",
    "\n",
    "        print(save_sent)\n",
    "        return max_sentence_length\n",
    "\n",
    "    def get_min_sentence_length(self):\n",
    "\n",
    "        data = json.load(open(self.path_to_file))\n",
    "        annotations = data['annotations']\n",
    "        save_sent = []\n",
    "        min_sentence_length = 10\n",
    "\n",
    "        for annotation in annotations:\n",
    "            sentence = annotation[0]['text'].split()\n",
    "            length = len(sentence)\n",
    "            if (length < min_sentence_length):\n",
    "                min_sentence_length = length\n",
    "                save_sent = sentence\n",
    "\n",
    "        print(save_sent)\n",
    "        return min_sentence_length\n",
    "\n",
    "    def sentences_to_index(self, vocabulary_file=r'vist2017_vocabulary.json',\n",
    "                           image_embedding_file=r'myCNN_image_features_val.hdf5',\n",
    "                           save_file_path=r'stories_to_val_index_myCNN.hdf5',\n",
    "                           images_directory=r'C:\\Users\\Nishanth\\ai-visual-storytelling-seq2seq\\vist_dataset\\images\\val',\n",
    "                           max_length=20):\n",
    "\n",
    "        vocabulary = json.load(open(vocabulary_file))\n",
    "        \n",
    "        #print(vocabulary)\n",
    "        \n",
    "        self.words_to_idx = vocabulary[\"words_to_idx\"]\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        #print(self.path_to_file)\n",
    "        \n",
    "        data = json.load(open(self.path_to_file))\n",
    "        annotations = data[\"annotations\"]\n",
    "        \n",
    "        \n",
    "        descriptions = self.descriptions_to_index()\n",
    "        img_hash = self.get_image_features_hash(image_embedding_file)\n",
    "        print(img_hash)\n",
    "        \n",
    "        \n",
    "        images_path_names = [y for y in Path(r'C:\\Users\\Nishanth\\ai-visual-storytelling-seq2seq\\vist_dataset\\images\\val',\n",
    ").rglob('*.jpg')]\n",
    "        images_path_names_png = [y for y in Path(r'C:\\Users\\Nishanth\\ai-visual-storytelling-seq2seq\\vist_dataset\\images\\val',\n",
    ").rglob('*.png')]\n",
    "        #print(images_path_names)\n",
    "        images_path_names = np.append(images_path_names, images_path_names_png)\n",
    "        print(images_path_names)\n",
    "\n",
    "        story_ids = []\n",
    "        story_sentences = []\n",
    "        story_images = []\n",
    "        story_images_ids = []\n",
    "        story_descriptions = []\n",
    "        story_images_paths = []\n",
    "\n",
    "        for i in range(0, len(annotations),5):\n",
    "\n",
    "            var = (int(annotations[i][0]['photo_flickr_id']) in descriptions.keys()) and (\n",
    "                    int(annotations[i + 1][0]['photo_flickr_id']) in descriptions.keys()) and (\n",
    "                          int(annotations[i + 2][0]['photo_flickr_id']) in descriptions.keys()) and (\n",
    "                          int(annotations[i + 3][0]['photo_flickr_id']) in descriptions.keys()) and (\n",
    "                          int(annotations[i + 4][0]['photo_flickr_id']) in descriptions.keys())\n",
    "\n",
    "            if not var:\n",
    "                continue\n",
    "\n",
    "            story_id = annotations[i][0][\"story_id\"]\n",
    "            #print(story_id)\n",
    "            img_id1, order1 = int(annotations[i][0][\"photo_flickr_id\"]), annotations[i][0][\n",
    "                \"worker_arranged_photo_order\"]\n",
    "            img_id2, order2 = int(annotations[i + 1][0][\"photo_flickr_id\"]), annotations[i + 1][0][\n",
    "                \"worker_arranged_photo_order\"]\n",
    "            img_id3, order3 = int(annotations[i + 2][0][\"photo_flickr_id\"]), annotations[i + 2][0][\n",
    "                \"worker_arranged_photo_order\"]\n",
    "            img_id4, order4 = int(annotations[i + 3][0][\"photo_flickr_id\"]), annotations[i + 3][0][\n",
    "                \"worker_arranged_photo_order\"]\n",
    "            img_id5, order5 = int(annotations[i + 4][0][\"photo_flickr_id\"]), annotations[i + 4][0][\n",
    "                \"worker_arranged_photo_order\"]\n",
    "            \n",
    "            #print(img_id1)\n",
    "\n",
    "            if not (str(img_id1) in img_hash):\n",
    "                continue\n",
    "            else:\n",
    "                image1 = img_hash[str(img_id1)]\n",
    "\n",
    "            if not (str(img_id2) in img_hash):\n",
    "                continue\n",
    "            else:\n",
    "                image2 = img_hash[str(img_id2)]\n",
    "\n",
    "            if not (str(img_id3) in img_hash):\n",
    "                continue\n",
    "            else:\n",
    "                image3 = img_hash[str(img_id3)]\n",
    "\n",
    "            if not (str(img_id4) in img_hash):\n",
    "                continue\n",
    "            else:\n",
    "                image4 = img_hash[str(img_id4)]\n",
    "\n",
    "            if not (str(img_id5) in img_hash):\n",
    "                continue\n",
    "            else:\n",
    "                image5 = img_hash[str(img_id5)]\n",
    "\n",
    "            story1 = self.sentences_to_index_helper(annotations[i][0][\"text\"], self.words_to_idx, max_length)\n",
    "            #print(story1)\n",
    "            story2 = self.sentences_to_index_helper(annotations[i + 1][0][\"text\"], self.words_to_idx, max_length)\n",
    "            story3 = self.sentences_to_index_helper(annotations[i + 2][0][\"text\"], self.words_to_idx, max_length)\n",
    "            story4 = self.sentences_to_index_helper(annotations[i + 3][0][\"text\"], self.words_to_idx, max_length)\n",
    "            story5 = self.sentences_to_index_helper(annotations[i + 4][0][\"text\"], self.words_to_idx, max_length)\n",
    "\n",
    "            description1 = descriptions[int(annotations[i][0]['photo_flickr_id'])]\n",
    "            #print(description1)\n",
    "            description2 = descriptions[int(annotations[i + 1][0]['photo_flickr_id'])]\n",
    "            description3 = descriptions[int(annotations[i + 2][0]['photo_flickr_id'])]\n",
    "            description4 = descriptions[int(annotations[i + 3][0]['photo_flickr_id'])]\n",
    "            description5 = descriptions[int(annotations[i + 4][0]['photo_flickr_id'])]\n",
    "\n",
    "            order1 = annotations[i][0][\"worker_arranged_photo_order\"]\n",
    "            #print(order1)\n",
    "            order2 = annotations[i + 1][0][\"worker_arranged_photo_order\"]\n",
    "            order3 = annotations[i + 2][0][\"worker_arranged_photo_order\"]\n",
    "            order4 = annotations[i + 3][0][\"worker_arranged_photo_order\"]\n",
    "            order5 = annotations[i + 4][0][\"worker_arranged_photo_order\"]\n",
    "\n",
    "            story_list = [(story1, order1), (story2, order2), (story3, order3), (story4, order4), (story5, order5)]\n",
    "            story_list = sorted(story_list, key=operator.itemgetter(1))\n",
    "            #print(story_list)\n",
    "            image_list = [(image1, order1), (image2, order2), (image3, order3), (image4, order4), (image5, order5)]\n",
    "            image_list = sorted(image_list, key=operator.itemgetter(1))\n",
    "\n",
    "            ordered_stories = [story_list[0][0], story_list[1][0], story_list[2][0], story_list[3][0], story_list[4][0]]\n",
    "            ordered_images = [image_list[0][0], image_list[1][0], image_list[2][0], image_list[3][0], image_list[4][0]]\n",
    "            ordered_image_ids = [img_id1, img_id2, img_id3, img_id4, img_id5]\n",
    "            print(ordered_stories)\n",
    "            print(ordered_images)\n",
    "            print(ordered_image_ids)\n",
    "\n",
    "            ordered_descriptions = [description1, description2, description3, description4, description5]\n",
    "\n",
    "            #ordered_image_path_names = []\n",
    "            #for file_idx in range(len(images_path_names)):\n",
    "             #   if images_path_names[file_idx].find(str(img_id1)):\n",
    "              #      ordered_image_path_names.append(images_path_names[file_idx])\n",
    "               # elif images_path_names[file_idx].find(str(img_id2)):\n",
    "               #     ordered_image_path_names.append(images_path_names[file_idx])\n",
    "               # elif images_path_names[file_idx].find(str(img_id3)):\n",
    "                #    ordered_image_path_names.append(images_path_names[file_idx])\n",
    "              #  elif images_path_names[file_idx].find(str(img_id4)):\n",
    "               #     ordered_image_path_names.append(images_path_names[file_idx])\n",
    "               # elif images_path_names[file_idx].find(str(img_id5)):\n",
    "                #    ordered_image_path_names.append(images_path_names[file_idx])\n",
    "               # else:\n",
    "                #    ordered_image_path_names.append(\"None\")\n",
    "\n",
    "            story_ids.append(int(story_id))\n",
    "            story_sentences.append(ordered_stories)\n",
    "            story_images.append(ordered_images)\n",
    "            story_images_ids.append(ordered_image_ids)\n",
    "            story_descriptions.append(ordered_descriptions)\n",
    "            # story_images_paths.append(ordered_image_path_names)\n",
    "\n",
    "        print(story_ids)\n",
    "        print(story_sentences)\n",
    "        print(story_images)\n",
    "        print(story_images_ids)\n",
    "        print(story_descriptions)\n",
    "        #print(story_images_paths)\n",
    "        \n",
    "        data_file = h5py.File(save_file_path, 'w')\n",
    "        data_file.create_dataset(\"story_ids\", data=story_ids)\n",
    "        data_file.create_dataset(\"story_sentences\", data=story_sentences)\n",
    "        data_file.create_dataset(\"image_embeddings\", data=story_images)\n",
    "        data_file.create_dataset(\"image_ids\", data=story_images_ids)\n",
    "        data_file.create_dataset(\"descriptions\", data=story_descriptions)\n",
    "        data_file.create_dataset(\"image_paths\", data = story_images_paths)\n",
    "\n",
    "    def descriptions_to_index(self):\n",
    "        description_data = json.load(\n",
    "            open(r'val.description-in-isolation.json'))['annotations']\n",
    "\n",
    "        description_to_index = {}\n",
    "        for i in range(len(description_data)):\n",
    "            photo_id = int(description_data[i][0]['photo_flickr_id'])\n",
    "            if photo_id not in description_to_index.keys():\n",
    "                description_to_index[photo_id] = self.sentences_to_index_helper(description_data[i][0]['text'],\n",
    "                                                                                self.words_to_idx, 20)\n",
    "\n",
    "        return description_to_index\n",
    "\n",
    "    def sentences_to_index_helper(self, sentence, word_to_idx, max_length):\n",
    "        words = sentence.split()\n",
    "        result_sentence = []\n",
    "\n",
    "        for word in words:\n",
    "            if len(result_sentence) == max_length:\n",
    "                break\n",
    "            else:\n",
    "                if word in word_to_idx:\n",
    "                    result_sentence.append(word_to_idx[word])\n",
    "                else:\n",
    "                    result_sentence.append(word_to_idx[\"<UNK>\"])\n",
    "\n",
    "        result_sentence.insert(0, word_to_idx[\"<START>\"])\n",
    "        result_sentence.append(word_to_idx[\"<END>\"])\n",
    "\n",
    "        while len(result_sentence) < max_length + 2:\n",
    "            result_sentence.append(word_to_idx[\"<NULL>\"])\n",
    "\n",
    "        return result_sentence\n",
    "\n",
    "    def indecies_to_sentence(self, sentence, idx_to_word):\n",
    "\n",
    "        result_sentence = \"\"\n",
    "        for word in sentence:\n",
    "            if word == 0:\n",
    "                result_sentence = result_sentence + \" \" + idx_to_word[word]\n",
    "\n",
    "        print(result_sentence)\n",
    "        return result_sentence\n",
    "\n",
    "    def get_image_features_hash(self, file_name):\n",
    "        L = []\n",
    "        image_features_file = h5py.File(file_name, 'r')\n",
    "        image_features_ids = image_features_file[\"image_ids\"]\n",
    "        image_embeddings = image_features_file[\"embeddings\"]\n",
    "        dictionary = {}\n",
    "        \n",
    "        for i in image_features_ids:\n",
    "            L.append(i.decode('unicode_escape'))\n",
    "\n",
    "        \n",
    "        \n",
    "        for id, em in zip(L, image_embeddings):\n",
    "            dictionary[str(id)] = em\n",
    "\n",
    "        return dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "object = SIS_DataReader()\n",
    "object.create_word_frequency_document()\n",
    "object.generate_vocabulary()\n",
    "object.sentences_to_index()\n",
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: KeysView(<HDF5 file \"stories_to_train_index_myCNN.hdf5\" (mode r)>)\n",
      "['descriptions', 'image_embeddings', 'image_ids', 'image_paths', 'story_ids', 'story_sentences']\n",
      "<HDF5 dataset \"descriptions\": shape (5287, 5, 22), type \"<i4\">\n"
     ]
    }
   ],
   "source": [
    "filename = r'C:\\Users\\Nishanth\\Desktop\\ai-visual-storytelling-seq2seq\\stories_to_train_index_myCNN.hdf5'\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    print(list(f.keys()))\n",
    "    print(f['descriptions'])\n",
    "    a_group_key = list(f.keys())[2] #image_id\n",
    "\n",
    "    # Get the data\n",
    "    data1 = list(f[a_group_key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2752715, 2827527, 2827386, 2827303, 2827247], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6612392275852992\n",
      "0.19049864185045087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishanth\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\Nishanth\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "#1 Four people posing for a picture, there are three females and one male\n",
    "#2 Group of people dancing in a darkly lit room\n",
    "#3 man with a headband and glasses standing with a bottle in his hand\n",
    "#4 Man with headband and glasses sitting atop the shoulders of another man who is sticking his tongue out\n",
    "#5 A photo of three people with the female in the front sticking out her tongue\n",
    "\n",
    "# our model\n",
    "#1 we all had a party at the pub\n",
    "#2 we invited all of my friends at the party\n",
    "#3 here we are having a lot of fun playing games \n",
    "#4 and then there was some fun playing with pool  \n",
    "#5 but at the end of the night , we all got together and went home all over it  \n",
    "\n",
    "# alexnet\n",
    "#1 we all had a little good family last night\n",
    "#2 we had to make it all in the night and i hope we do \n",
    "#3 we then went back home and had some good conversation\n",
    "#4 after that dinner , we played some video games \n",
    "#5 some of my friends got a little crazy\n",
    "\n",
    "hypothesis = ['four', 'people', 'posing', 'for', 'a', 'picture','there','are','three','females','and','one','male','group','of','people','dancing','in','a','darkly','lit','room','man','with', 'a','headband','and','glasses','standing','with','a','bottle','in','his','hand','Man','with','headband','and','glasses','sitting','atop','the','shoulders','of','another','man','who','is','sticking','his','tongue','out','A','photo','of','three','people','with','the','female','in','the','front','sticking','out','her','tongue']\n",
    "reference = ['we','all','had','a','party','at','the','pub','we','invited','all','of','my','friends','at','the','party','here','we','are','having','a','lot','of','fun','playing','games','and','then','there','was','some','fun','playing','with','pool','but','at','the','end','of','the','night','we','all','got','together','and','went','home','all','over','it']\n",
    "reference_1 = ['we','all','had','a','little','good','family','last','night','we','had','to','make','it','all','in','the','night','and','i','hope','we','do','after','that','dinner','we','played','some','video','games','some','of','my','friends','got','a','little','crazy']\n",
    "#there may be several references\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "BLEUscore_1 = nltk.translate.bleu_score.sentence_bleu([reference_1], hypothesis)\n",
    "\n",
    "print(BLEUscore)\n",
    "print(BLEUscore_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35459569487016757\n",
      "0.3295645115332452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishanth\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "#original\n",
    "#1 A bunch of people at a Christmas party only one woman is sitting down\n",
    "#2 The counter top has several plates of food on it\n",
    "#3 A set of plates that are on the kitchen counter\n",
    "#4 Different people gather at the table during the holidays\n",
    "#5 Three young adults are socializing in the living room of a home around Christmas time\n",
    "\n",
    "#our model\n",
    "#1 the whole family got together for a christmas party\n",
    "#2 my sister was invited to a $<$UNK$>$ for the barbecue so everyone had fun \n",
    "#3 there was quite a lot of food and drinks\n",
    "#4 and here we are at the party started getting ready and ready for dinner \n",
    "#5 then we had a group photo , everyone in the kitchen we set up \n",
    "\n",
    "#alexnet\n",
    "#1 i love to travel\n",
    "#2 i a good kitchen with fresh tomatoes and different types of food\n",
    "#3 i have a lot of food i put my best of me i see i look at this well !\n",
    "#4 here we are $<$UNK$>$ at the kitchen table eating , so !\n",
    "#5 then we were dancing to the music\n",
    "\n",
    "hypothesis = ['A','bunch','of','people','at','a','Christmas','party','only','one','woman','is','sitting','down','The','counter','top','has','several','plates','of','food','on','it','A','set','of','plates','that','are','on','the','kitchen','counter','Different','people','gather','at','the','table','during','the','holidays','Three','young','adults','are','socializing','in','the','living','room','of','a','home','around','Christmas','time']\n",
    "reference = ['the','whole','family','got','together','for','a','christmas','party','my','sister','was','invited','to','a','UNK','for','the','barbecue','so','everyone','had','fun','there','was','quite','a','lot','of','food','and','drinks','and','here','we','are','at','the','party','started','getting','ready','and','ready','for','dinner','then','we','had','a','group','photo','everyone','in','the','kitchen','we','set','up']\n",
    "reference_1 = ['i','love','to','travel','i','a','good','kitchen','with','fresh','tomatoes','and','different','types','of','food','i','have','a','lot','of','food','i','put','my','best','of','me','i','see','i','look','at','this','well','here','we','are','UNK','at','the','kitchen','table','eating','so','then','we','were','dancing','to','the','music']\n",
    "\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "BLEUscore_1 = nltk.translate.bleu_score.sentence_bleu([reference_1], hypothesis)\n",
    "\n",
    "print(BLEUscore)\n",
    "print(BLEUscore_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19365933250318007\n",
      "0.6324555320336759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishanth\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\Nishanth\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "#original\n",
    "#1 A group of youngsters are ice skating on the pond\n",
    "#2 A group of young women on ice skates during winter\n",
    "#3 a woman is balancing on ice skates on a rink\n",
    "#4 this ice skater performs a flawless manuever here\n",
    "#5 The children are wearing hockey skates and skating at an ice rink\n",
    "\n",
    "#our model\n",
    "#1 at the ice skating festival , people stood around and spectators\n",
    "#2 they linked t-shirts and skated around and enjoyed the show\n",
    "#3 this girl also had a great time with the ball but he could n't really get a little worried\n",
    "#4 as , $<$UNK$>$ , another with the bridesmaids you tell him know he was n't to help\n",
    "#5 children and friends were waiting outside the church\n",
    "\n",
    "#alex\n",
    "#1 at the local skating park , there are many attractions\n",
    "#2 they linked each other together\n",
    "#3 the girl in yellow knew how to do tricks\n",
    "#4 as white , white hair of white shirt is even a major\n",
    "#5 here is the main owners arrived to the location \n",
    "\n",
    "hypothesis = ['A','group','of','youngsters','are','ice','skating','on','the','pond','A','group','of','young','women','on','ice','skates','during','winter','a','woman','is','balancing','on','ice','skates','on','a','rink','this','ice','skater','performs','a','flawless','manuever','here','The','children','are','wearing','hockey','skates','and','skating','at','an','ice','rink']\n",
    "reference = ['at','the','ice','skating','festival','people','stood','around','and','spectators','they','linked','t-shirts','and','skated','around','and','enjoyed','the','show','this','girl','also','had','a','great','time','with','the','ball','but','he','could','not','really','get','a','little','worried','as','UNK','another','with','the','bridesmaids','you','tell','him','know','he','was','not','to','help','children','and','friends','were','waiting','outside','the','church']\n",
    "reference_1 = ['at','the','local','skating','park','there','are','many','attractions','they','linked','each','other','together','the','girl','in','yellow','knew','how','to','do','tricks','as','white','white','hair','of','white','shirt','is','even','a','major','here','is','the','main','owners','arrived','to','the','location']\n",
    "\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "BLEUscore_1 = nltk.translate.bleu_score.sentence_bleu([reference_1], hypothesis)\n",
    "\n",
    "print(BLEUscore)\n",
    "print(BLEUscore_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4768521208065966\n",
      "0.492057143452666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishanth\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "#original\n",
    "#1 Three children (a girl and two boys) walking together outdoors, holding hands\n",
    "#2 three boys are sitting together on a green chair\n",
    "#3 Two children are reaching into a basket for necklaces\n",
    "#4 The children are wearing necklaces on their neck\n",
    "#5 Two women with children are sitting by a mailbox\n",
    "\n",
    "#our model\n",
    "#1 the man was happy\n",
    "#2 they take a moment to take a picture with the kids\n",
    "#3 at the reception , the children decided to spend a day out in the country\n",
    "#4 everyone had a great time at the reception , everyone was waiting for them\n",
    "#5 and [female] had a blast and was ready to eat a delicious meal and before the year was going\n",
    "\n",
    "#alexnet\n",
    "#1 school company day . a new $<$UNK$>$ of $<$UNK$>$ $<$UNK$>$} \n",
    "#2 she was so excited to be there\n",
    "#3 after the ceremony everyone had her speech himself using the entire bottle\n",
    "#4 she took a picture with them from the goats\n",
    "#5 she she walked her down the photographer he would she would\n",
    "\n",
    "hypothesis = ['Three','children','a','girl','and','two','boys','walking','together','outdoors','holding','hands','three','boys','are','sitting','together','on','a','green','chair','Two','children','are','reaching','into','a','basket','for','necklaces','The','children','are','wearing','necklaces','on','their','neck','Two','women','with','children','are','sitting','by','a','mailbox']\n",
    "reference = ['the','man','was','happy','they','take','a','moment','to','take','a','picture','with','the','kids','at','the','reception','the','children','decided','to','spend','a','day','out','in','the','country','everyone','had','a','great','time','at','the','reception','everyone','was','waiting','for','them','and','female','had','a','blast','and','was','ready','to','eat','a','delicious','meal','and','before','the','year','was','going']\n",
    "reference_1 = ['school','company','day','a','new','UNK','of','UNK','UNK','she','was','so','excited','to','be','there','after','the','ceremony','everyone','had','her','speech','himself','using','the','entire','bottle','she','took','a','picture','with','them','from','the','goats','she','she','walked','her','down','the','photographer','he','would','she','would']\n",
    "\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "BLEUscore_1 = nltk.translate.bleu_score.sentence_bleu([reference_1], hypothesis)\n",
    "\n",
    "print(BLEUscore)\n",
    "print(BLEUscore_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2496385375070295\n",
      "0.6632807437760121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishanth\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\Nishanth\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "#original\n",
    "#1 A boy that is wearing no clothes that is standing next to the tree and his brother\n",
    "#2 Two children are having fun building a gingerbread house\n",
    "#3 A gingerbread house that is on the table\n",
    "#4 A boy at christmas time posing with a man dressed as santa\n",
    "#5 Two young boys standing together and holding a Wii video game system, still in the box\n",
    "\n",
    "#our model\n",
    "#1 the man in green was pretty well in the morning\n",
    "#2 after that he met up in the snow covered that was amazing\n",
    "#3 the tree that contained had been created for the occasion\n",
    "#4 of course $[$male$]$ was not happy that he had to leave and tell what a great time when it was going\n",
    "#5 they could n't think the love and do n't wait to do this time , but , all of their memories\n",
    "\n",
    "#alex\n",
    "#1 it was christmas night and i had a lot of fun\n",
    "#2 after that we found a small store that had lots of alcohol\n",
    "#3 it was bad enough comfortable\n",
    "#4 of course barb knew he would be seen a few words from her best stories to see\n",
    "#5 after all the presents had been opened , the boys were talking about making a smile\n",
    "\n",
    "\n",
    "hypothesis = ['A','boy','that','is','wearing','no','clothes','that','is','standing','next','to','the','tree','and','his','brother','Two','children','are','having','fun','building','a','gingerbread','house','A','gingerbread','house','that','is','on','the','table','A','boy','at','christmas','time','posing','with','a','man','dressed','as','santa','Two','young','boys','standing','together','and','holding','a','Wii','video','game','system','still','in','the','box']\n",
    "reference = ['the','man','in','green','was','pretty','well','in','the','morning','after','that','he','met','up','in','the','snow','covered','that','was','amazing','the','tree','that','contained','had','been','created','for','the','occasion','of','course','male','was','not','happy','that','he','had','to','leave','and','tell','what','a','great','time','when','it','was','going','they','could','not','think','the','love','and','do','not','wait','to','do','this','time','but','all','of','their','memories']\n",
    "reference_1 = ['it','was','christmas','night','and','i','had','a','lot','of','fun','after','that','we','found','a','small','store','that','had','lots','of','alcohol','it','was','bad','enough','comfortable','of','course','barb','knew','he','would','be','seen','a','few','words','from','her','best','stories','to','see','after','all','the','presents','had','been','opened','the','boys','were','talking','about','making','a','smile']\n",
    "\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "BLEUscore_1 = nltk.translate.bleu_score.sentence_bleu([reference_1], hypothesis)\n",
    "\n",
    "print(BLEUscore)\n",
    "print(BLEUscore_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18653567503517982\n",
      "0.2242275002250832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishanth\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "#original\n",
    "#1 A bird latches onto a tree trunk to find itself food\n",
    "#2 Tall pine trees stand tall above the white snow\n",
    "#3 A mountain lake scene with clouds in the sky\n",
    "#4 Multiple berries on a branch surrounded by some leaves\n",
    "#5 A bunch of buildings sit in the middle of a field\n",
    "\n",
    "#our model\n",
    "#1 claus the woodpecker was searching for berries\n",
    "#2 he searches through the trees that sat in the tree\n",
    "#3 when the sun went over the coliseum , we decided to go inside the hill and really beautiful going at\n",
    "#4 here i am enjoying a couple who i wish that they were able to take a picture and wish to have with\n",
    "#5 here i took a picture of a wall with a castle from the east of the town just $<$UNK$>\n",
    "\n",
    "#alex\n",
    "#1 claus the woodpecker was searching for secret\n",
    "#2 he searches through the trees , grass and trees\n",
    "#3 he found trees the tall trees trees as the sky makes its way to the sky\n",
    "#4 these grapes even pretty enough to eat the delicious delicious\n",
    "#5 the woodpecker was sad when i went to the organization\n",
    "\n",
    "hypothesis = ['A','bird','latches','onto','a','tree','trunk','to','find','itself','food','Tall','pine','trees','stand','tall','above','the','white','snow','A','mountain','lake','scene','with','clouds','in','the','sky','Multiple','berries','on','a','branch','surrounded','by','some','leaves','A','bunch','of','buildings','sit','in','the','middle','of','a','field']\n",
    "reference = ['claus','the','woodpecker','was','searching','for','berries','he','searches','through','the','trees','that','sat','in','the','tree','when','the','sun','went','over','the','coliseum','we','decided','to','go','inside','the','hill','and','really','beautiful','going','at','here','i','am','enjoying','a','couple','who','i','wish','that','they','were','able','to','take','a','picture','and','wish','to','have','with','here','i','took','a','picture','of','a','wall','with','a','castle','from','the','east','of','the','town','just','UNK']\n",
    "reference_1 = ['claus','the','woodpecker','was','searching','for','secret','he','searches','through','the','trees','grass','and','trees','he','found','trees','the','tall','trees','trees','as','the','sky','makes','its','way','to','the','sky','these','grapes','even','pretty','enough','to','eat','the','delicious','delicious','the','woodpecker','was','sad','when','i','went','to','the','organization']\n",
    "\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "BLEUscore_1 = nltk.translate.bleu_score.sentence_bleu([reference_1], hypothesis)\n",
    "\n",
    "print(BLEUscore)\n",
    "print(BLEUscore_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
